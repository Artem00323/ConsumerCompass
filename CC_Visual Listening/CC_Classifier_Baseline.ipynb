{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7FeSVhUruTy",
    "outputId": "cf40e6ea-2820-4546-9939-32ed11fa16c0"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iZF05THmi6N"
   },
   "source": [
    "# Splitting for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMUaf_STl9nM",
    "outputId": "c1f9dbb9-0813-463f-aa86-bafc2a63f37d"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img_path, target_size=(224, 224)):\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img.astype('float32') / 255.0\n",
    "    return img\n",
    "\n",
    "# Google drive (if colab)\n",
    "# base_folder = '/content/drive/My Drive/ConsumerCompass/data'\n",
    "\n",
    "base_folder = 'data 2' # insert your path\n",
    "\n",
    "folders = [os.path.join(base_folder, folder_name) for folder_name in ['healthy', 'unhealthy', 'glamorous', 'drab', 'rugged', 'gentle', 'fun', 'dull',\n",
    "                                                                      'innovation', 'conservatism', 'premuim', 'accessibility', 'minimalism', 'detail', 'safety', 'risk']]\n",
    "\n",
    "#class lables \n",
    "label_map = {'healthy': 0, 'unhealthy': 1, 'glamorous': 2, 'drab': 3, 'rugged': 4, 'gentle': 5, 'fun': 6, 'dull': 7,\n",
    "             'innovation': 8, 'conservatism': 9, 'premuim': 10, 'accessibility': 11, 'minimalism': 12, 'detail': 13, 'safety': 14, 'risk': 15}\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "# Dataframe creation \n",
    "for folder in folders:\n",
    "    label_index = label_map[os.path.basename(folder)]\n",
    "    print(folder)\n",
    "    for img_file in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img_file)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = preprocess_image(img_path)\n",
    "            label_vector = [0] * 16 # number of lables\n",
    "            label_vector[label_index] = 1\n",
    "            all_images.append(img)\n",
    "            all_labels.append(label_vector)\n",
    "\n",
    "all_images = np.array(all_images)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(all_images, all_labels, test_size=0.3, random_state=42)\n",
    "val_images, test_images, val_labels, test_labels = train_test_split(test_images, test_labels, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCh8KbFTmmBG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    # Conv1\n",
    "    Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), activation='relu', input_shape=(224, 224, 3)),\n",
    "    # MaxPool1\n",
    "    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    # Norm1\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Conv2\n",
    "    Conv2D(filters=256, kernel_size=(5, 5), padding='same', activation='relu'),\n",
    "    # MaxPool2\n",
    "    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    # Norm2\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Conv3\n",
    "    Conv2D(filters=384, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "\n",
    "    # Conv4\n",
    "    Conv2D(filters=384, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "\n",
    "    # Conv5\n",
    "    Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    # MaxPool5\n",
    "    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    # FC6\n",
    "    Dense(4096, activation='relu'),\n",
    "    # Dropout FC6\n",
    "    Dropout(0.5),\n",
    "\n",
    "    # FC7\n",
    "    Dense(4096, activation='relu'),\n",
    "    # Dropout FC7\n",
    "    Dropout(0.5),\n",
    "\n",
    "    # FC8\n",
    "    Dense(16, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "#fit the model\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "\n",
    "model.save('/content/model_multilabel_brandimagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsPor_TmmmD9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "accuracy_metrics = []\n",
    "for i in range(0, predictions.shape[1], 2):\n",
    "    metric = BinaryAccuracy()\n",
    "    metric.update_state(test_labels[:, i:i+2], predictions[:, i:i+2])\n",
    "    accuracy_metrics.append(metric.result().numpy())\n",
    "\n",
    "class_pairs = ['healthy-unhealthy', 'glamorous-drab', 'rugged-gentle', 'fun-dull',\n",
    "               'innovation-conservatism', 'premuim-accessibility', 'minimalism-detail', 'safety-risk']\n",
    "\n",
    "#print accuracy for every single pair\n",
    "for i, acc in enumerate(accuracy_metrics):\n",
    "    print(f\"Accuracy for {class_pairs[i]}: {acc}\")\n",
    "\n",
    "#print avg accuracy \n",
    "average_accuracy = np.mean(accuracy_metrics)\n",
    "print(f\"Average Accuracy across all class pairs: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tp_fp_tn_fn_per_pair(model, test_images, test_labels):\n",
    "    predictions = model.predict(test_images)\n",
    "    predictions_binary = (predictions > 0.5).astype(int)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for i in range(0, test_labels.shape[1], 2):\n",
    "        # filtering images for every pair\n",
    "        relevant_indices = (test_labels[:, i] + test_labels[:, i + 1] > 0)\n",
    "        relevant_labels = test_labels[relevant_indices, i:i+2]\n",
    "        relevant_predictions = predictions_binary[relevant_indices, i:i+2]\n",
    "\n",
    "        tp_metric = tf.keras.metrics.TruePositives()\n",
    "        fp_metric = tf.keras.metrics.FalsePositives()\n",
    "        tn_metric = tf.keras.metrics.TrueNegatives()\n",
    "        fn_metric = tf.keras.metrics.FalseNegatives()\n",
    "        \n",
    "        #calculate tp fp tn fn for inside pair\n",
    "        tp_metric.update_state(relevant_labels, relevant_predictions)\n",
    "        fp_metric.update_state(relevant_labels, relevant_predictions)\n",
    "        tn_metric.update_state(relevant_labels, relevant_predictions)\n",
    "        fn_metric.update_state(relevant_labels, relevant_predictions)\n",
    "        \n",
    "        results.append({\n",
    "            'TP': tp_metric.result().numpy(),\n",
    "            'FP': fp_metric.result().numpy(),\n",
    "            'TN': tn_metric.result().numpy(),\n",
    "            'FN': fn_metric.result().numpy()\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "results = calculate_tp_fp_tn_fn_per_pair(model, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate precision for pair\n",
    "def calculate_precision(results):\n",
    "    precisions = []\n",
    "    for result in results:\n",
    "        tp = result['TP']\n",
    "        fp = result['FP']\n",
    "        if tp + fp == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "        precisions.append(precision)\n",
    "    return precisions\n",
    "\n",
    "#calculate precision for every pair\n",
    "precisions = calculate_precision(results)\n",
    "\n",
    "#calculate and print avg precision\n",
    "average_precision = np.mean(precisions)\n",
    "print(f\"Average Precision across all class pairs: {average_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oZNeF6pmmGj"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# model = load_model('/content/model2')\n",
    "\n",
    "# image_path = '/content/test/pic1.jpg' # upload to google collab manually\n",
    "\n",
    "# img = preprocess_image(image_path)\n",
    "\n",
    "# img_batch = np.expand_dims(img, axis=0)\n",
    "\n",
    "# predictions = model.predict(img_batch)\n",
    "\n",
    "# final_classification = []\n",
    "\n",
    "# for i in range(0, len(predictions[0]), 2):\n",
    "#     if predictions[0][i] > predictions[0][i+1]:\n",
    "#         final_classification.extend([1, 0])\n",
    "#     else:\n",
    "#         final_classification.extend([0, 1])\n",
    "\n",
    "# print(final_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elFYDiNDmmKf"
   },
   "outputs": [],
   "source": [
    "# queries = [\"healthy\", \"unhealthy\", \"glamorous\", \"drab\",\n",
    "#            \"rugged\", \"gentle\", \"fun\", \"dull\",\n",
    "#            'innovation', 'conservatism', 'premuim', 'accessibility',\n",
    "#            'minimalism', 'detail', 'safety', 'risk']\n",
    "\n",
    "# def normalize_and_print_predictions(predictions):\n",
    "#     normalized_predictions = []\n",
    "\n",
    "#     for i in range(0, len(predictions[0]), 2):\n",
    "#         pair_sum = predictions[0][i] + predictions[0][i+1]\n",
    "#         normalized_pair = [predictions[0][i] / pair_sum, predictions[0][i+1] / pair_sum]\n",
    "#         normalized_predictions.extend(normalized_pair)\n",
    "\n",
    "#     # Print the normalized predictions\n",
    "#     for i in range(0, len(normalized_predictions), 2):\n",
    "#         print(f\"{queries[i]}: [{normalized_predictions[i]:.2f}, {normalized_predictions[i+1]:.2f}]\")\n",
    "\n",
    "# normalize_and_print_predictions(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nb3QIEnrFE3g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
